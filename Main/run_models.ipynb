{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dc87e2",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Models</a></span></li><li><span><a href=\"#Statistical-significance\" data-toc-modified-id=\"Statistical-significance-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Statistical significance</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f0862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from sklearn import linear_model\n",
    "from sklearn import feature_selection\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn as sklearn\n",
    "import multiprocessing\n",
    "import json\n",
    "import nni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f196f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f840946e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from keras import optimizers\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "#import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfe3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e524d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c6fa2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364dad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN parameters\n",
    "NUM_ITERATIONS = 300\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "LASSO_COEF = 50.0\n",
    "DECAY_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8540e3b5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "n_cores = multiprocessing.cpu_count() # Getting the number of cores for multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954c86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_process_regressor():\n",
    "    gp = GaussianProcessRegressor()\n",
    "    return [gp], ['Gaussian Process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38702171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_regressor(num_hidden_units=51):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=num_hidden_units)\n",
    "    return [mlp], ['Multi-Layer Perceptron']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9ad119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_models():\n",
    "    grad = GradientBoostingRegressor(n_estimators=17, random_state=42, loss='lad', learning_rate=0.12, max_depth=10)\n",
    "    classifier_list = [grad]\n",
    "    classifier_name_list = ['Gradient Boost']\n",
    "    return classifier_list, classifier_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_metrics(trained_model, trained_model_name, X_test, y_test):\n",
    "    print('--------- For Model: ', trained_model_name, ' ---------\\n')\n",
    "    predicted_values = trained_model.predict(X_test)\n",
    "    print(\"Mean absolute error: \",\n",
    "          metrics.mean_absolute_error(y_test, predicted_values))\n",
    "    print(\"Median absolute error: \",\n",
    "          metrics.median_absolute_error(y_test, predicted_values))\n",
    "    print(\"Mean squared error: \", metrics.mean_squared_error(\n",
    "        y_test, predicted_values))\n",
    "    print(\"R2: \", metrics.r2_score(y_test, predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99835dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e92dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_metrics2(trained_model, trained_model_name, X_test, y_test):\n",
    "    print('--------- For Model: ', trained_model_name, ' --------- (Train Data)\\n')\n",
    "    predicted_values = trained_model.predict(X_test)\n",
    "    print(\"Mean absolute error: \",\n",
    "          metrics.mean_absolute_error(y_test, predicted_values))\n",
    "    print(\"Median absolute error: \",\n",
    "          metrics.median_absolute_error(y_test, predicted_values))\n",
    "    print(\"Mean squared error: \", metrics.mean_squared_error(\n",
    "        y_test, predicted_values))\n",
    "    print(\"R2: \", metrics.r2_score(y_test, predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2135e68d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def svm(X_train, y_train, X_val, y_val):\n",
    "    model = SVR(gamma = 0.05, verbose = True) #was empty #0.1 #the - best gamma 0.05, c=0.5\n",
    "    model.fit(X_train, y_train)\n",
    "    print_evaluation_metrics(model, \"svm\", X_val, y_val.values.ravel())\n",
    "    print_evaluation_metrics2(model, \"svm\", X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03e1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearModel(X_train, y_train, X_val, y_val):\n",
    "    regr = linear_model.LinearRegression(n_jobs=int(0.8*n_cores)).fit(X_train, y_train)\n",
    "    print_evaluation_metrics(regr, \"linear model\", X_val, y_val.values.ravel())\n",
    "    print_evaluation_metrics2(regr, \"linear model\", X_train, y_train.values.ravel())\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e09306",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def LinearModelRidge(X_train, y_train, X_val, y_val):\n",
    "    regr = Ridge(alpha = 7) #7\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_val.values)\n",
    "\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    print(\"Mean squared error: %.2f\" % mean_squared_error(y_val, y_pred))\n",
    "    print('Variance score: %.2f' % r2_score(y_val, y_pred))\n",
    "    print(\"R2:\", sklearn.metrics.r2_score(y_val, y_pred))\n",
    "\n",
    "    print_evaluation_metrics(regr, \"Linear Model Ridge\", X_val, y_val)\n",
    "    print_evaluation_metrics2(regr, \"Linear Model Ridge\", X_train, y_train)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404d96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearModelLasso(X_train, y_train, X_val, y_val, alpha):\n",
    "    regr = Lasso(alpha = alpha) #0.5\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    print_evaluation_metrics(regr, \"Linear Model Lasso\", X_val, y_val)\n",
    "    print_evaluation_metrics2(regr, \"Linear Model Lasso\", X_train, y_train)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bd617c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neural_network(X_train, y_train, X_val, y_val):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=20, activation='relu', input_dim=len(X_train.values[0])))\n",
    "    model.add(Dense(units=5, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    adam = optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=DECAY_RATE, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "    model.fit(X_train, y_train, epochs=NUM_ITERATIONS, batch_size=BATCH_SIZE)\n",
    "    print(\"finished fitting\")\n",
    "    print_evaluation_metrics(model, \"NN\", X_val, y_val)\n",
    "    print_evaluation_metrics2(model, \"NN\", X_train, y_train)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3efcf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreebasedModel(X_train, y_train, X_val, y_val):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    y_train = np.squeeze(y_train)\n",
    "    y_val = np.squeeze(y_val)\n",
    "\n",
    "    classifier_list, classifier_name_list = get_ensemble_models()\n",
    "    for classifier, classifier_name in zip(classifier_list, classifier_name_list):\n",
    "        classifier.fit(X_train, y_train)\n",
    "        print_evaluation_metrics(classifier, classifier_name, X_val, y_val)\n",
    "        print_evaluation_metrics2(classifier, classifier_name, X_train, y_train)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdc0a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X_train, y_train, X_val, y_val):\n",
    "    n_clusters = 8\n",
    "    #kmeans = KMeans(n_clusters=n_clusters, random_state=0, verbose=0, n_jobs=int(0.8*n_cores)).fit(X_train)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, verbose=0).fit(X_train)\n",
    "    c_train = kmeans.predict(X_train)\n",
    "    c_pred = kmeans.predict(X_val)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    y_val_stats = None\n",
    "    predicted_values = None\n",
    "    y_train_stats = None\n",
    "    labels_stats = None\n",
    "    for i in range(n_clusters):\n",
    "        print('--------analyzing cluster %d--------' %i)\n",
    "        train_mask = c_train==i\n",
    "        std_train = np.std(y_train[train_mask])\n",
    "        mean_train = np.mean(y_train[train_mask])\n",
    "        print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
    "        pred_mask = c_pred==i\n",
    "        std_pred = np.std(y_val[pred_mask])\n",
    "        mean_pred = np.mean(y_val[pred_mask])\n",
    "        print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
    "        if pred_mask.sum() == 0:\n",
    "            print('Zero membered test set! Skipping the test and training validation.')\n",
    "            continue\n",
    "        #LinearModelRidge(X_train[train_mask], y_train[train_mask], X_val[pred_mask], y_val[pred_mask])\n",
    "        regr = Ridge(alpha = 7) #7\n",
    "        regr.fit(X_train[train_mask], y_train[train_mask])\n",
    "        labels_pred = regr.predict(X_train[train_mask].values)\n",
    "        y_pred = regr.predict(X_val[pred_mask].values)\n",
    "        if (y_val_stats is None):\n",
    "            y_val_stats = copy.deepcopy(y_val[pred_mask])\n",
    "            y_train_stats = copy.deepcopy(y_train[train_mask])\n",
    "            predicted_values = copy.deepcopy(y_pred)\n",
    "            labels_stats = copy.deepcopy(labels_pred)\n",
    "\n",
    "        else:\n",
    "            y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
    "            y_train_stats = y_train_stats.append(y_train[train_mask])\n",
    "            predicted_values = np.append(predicted_values, y_pred)\n",
    "            labels_stats = np.append(labels_stats, labels_pred)\n",
    "        print('--------Finished analyzing cluster %d--------' %i)\n",
    "    print(\"Mean absolute error: \",\n",
    "          metrics.mean_absolute_error(y_val_stats, predicted_values))\n",
    "    print(\"Median absolute error: \",\n",
    "          metrics.median_absolute_error(y_val_stats, predicted_values))\n",
    "    print(\"Mean squared error: \", metrics.mean_squared_error(\n",
    "        y_val_stats, predicted_values))\n",
    "    print(\"R2: \", metrics.r2_score(y_val_stats, predicted_values))\n",
    "    print('------------TRAIN--------------------')\n",
    "    print(\"Mean absolute error: \",\n",
    "        metrics.mean_absolute_error(y_train_stats, labels_stats))\n",
    "    print(\"Median absolute error: \",\n",
    "        metrics.median_absolute_error(y_train_stats, labels_stats))\n",
    "    print(\"Mean squared error: \", metrics.mean_squared_error(\n",
    "        y_train_stats, labels_stats))\n",
    "    print(\"R2: \", metrics.r2_score(y_train_stats, labels_stats))\n",
    "\n",
    "\n",
    "    return c_pred, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d12a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_SGD(X_train, y_train, X_val, y_val):\n",
    "    model = SGDRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    print_evaluation_metrics(model, \"sgd\", X_val, y_val.values.ravel())\n",
    "    print_evaluation_metrics2(model, \"sgd\", X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8381db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../Data/data_cleaned_train_comments_X.csv')\n",
    "y_train = pd.read_csv('../Data/data_cleaned_train_y.csv')\n",
    "\n",
    "X_val = pd.read_csv('../Data/data_cleaned_val_comments_X.csv')\n",
    "y_val = pd.read_csv('../Data/data_cleaned_val_y.csv')\n",
    "\n",
    "X_test = pd.read_csv('../Data/data_cleaned_test_comments_X.csv')\n",
    "y_test = pd.read_csv('../Data/data_cleaned_test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cbb2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc71c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if s == True:\n",
    "    coeffs = np.load('../Data/selected_coefs.npy')\n",
    "    col_set = set()\n",
    "    cherry_picked_list = [\n",
    "    'host_identity_verified',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accommodates',\n",
    "    'bathrooms',\n",
    "    'bedrooms',\n",
    "    'beds',\n",
    "    'guests_included',\n",
    "    'security_deposit',\n",
    "    'cleaning_fee',\n",
    "    'extra_people',\n",
    "    'number_of_reviews',\n",
    "    'review_scores_rating',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'reviews_per_month',\n",
    "    'comments',\n",
    "\n",
    "    ]\n",
    "    \n",
    "    for i in range(len(coeffs)):\n",
    "        if (coeffs[i]):\n",
    "            col_set.add(X_train.columns[i])\n",
    "    X_train = X_train[list(col_set)]\n",
    "    X_val = X_val[list(col_set)]\n",
    "    X_test = X_test[list(col_set)]\n",
    "\n",
    "    X_concat = pd.concat([X_train, X_val], ignore_index=True)\n",
    "    y_concat = pd.concat([y_train, y_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1322775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Tree-based Model--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- For Model:  Gradient Boost  ---------\n",
      "\n",
      "Mean absolute error:  0.3221680756948255\n",
      "Median absolute error:  0.25290180006538043\n",
      "Mean squared error:  0.19082347147345055\n",
      "R2:  0.5983872008849019\n",
      "--------- For Model:  Gradient Boost  --------- (Train Data)\n",
      "\n",
      "Mean absolute error:  0.23047799483672443\n",
      "Median absolute error:  0.15919804894709566\n",
      "Mean squared error:  0.12478578363199358\n",
      "R2:  0.7396295916010915\n",
      "--------------------KMeans Clustering--------------------\n",
      "--------analyzing cluster 0--------\n",
      "# examples & price mean & std for training set within cluster 0 is:(3482, 5.06, 0.62)\n",
      "# examples & price mean & std for validation set within cluster 0 is:(438, 5.06, 0.61)\n",
      "--------Finished analyzing cluster 0--------\n",
      "--------analyzing cluster 1--------\n",
      "# examples & price mean & std for training set within cluster 1 is:(5115, 4.68, 0.61)\n",
      "# examples & price mean & std for validation set within cluster 1 is:(589, 4.64, 0.58)\n",
      "--------Finished analyzing cluster 1--------\n",
      "--------analyzing cluster 2--------\n",
      "# examples & price mean & std for training set within cluster 2 is:(3732, 4.58, 0.60)\n",
      "# examples & price mean & std for validation set within cluster 2 is:(411, 4.56, 0.60)\n",
      "--------Finished analyzing cluster 2--------\n",
      "--------analyzing cluster 3--------\n",
      "# examples & price mean & std for training set within cluster 3 is:(4153, 4.38, 0.57)\n",
      "# examples & price mean & std for validation set within cluster 3 is:(412, 4.39, 0.60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for training set within cluster %d is:(%d, %.2f, %.2f)\" %(i, train_mask.sum(), np.float(mean_train), np.float(std_train)))\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  print(\"# examples & price mean & std for validation set within cluster %d is:(%d, %.2f, %.2f)\" %(i, pred_mask.sum(), np.float(mean_pred), np.float(std_pred)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Finished analyzing cluster 3--------\n",
      "--------analyzing cluster 4--------\n",
      "# examples & price mean & std for training set within cluster 4 is:(2617, 5.08, 0.70)\n",
      "# examples & price mean & std for validation set within cluster 4 is:(310, 5.05, 0.74)\n",
      "--------Finished analyzing cluster 4--------\n",
      "--------analyzing cluster 5--------\n",
      "# examples & price mean & std for training set within cluster 5 is:(2950, 4.82, 0.76)\n",
      "# examples & price mean & std for validation set within cluster 5 is:(312, 4.77, 0.69)\n",
      "--------Finished analyzing cluster 5--------\n",
      "--------analyzing cluster 6--------\n",
      "# examples & price mean & std for training set within cluster 6 is:(3116, 4.80, 0.83)\n",
      "# examples & price mean & std for validation set within cluster 6 is:(350, 4.76, 0.82)\n",
      "--------Finished analyzing cluster 6--------\n",
      "--------analyzing cluster 7--------\n",
      "# examples & price mean & std for training set within cluster 7 is:(3261, 4.75, 0.59)\n",
      "# examples & price mean & std for validation set within cluster 7 is:(336, 4.74, 0.65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_val_stats = y_val_stats.append(y_val[pred_mask])\n",
      "/var/folders/dh/pf9jys0d4_s_bxm_6zl1fbdh0000gn/T/ipykernel_1423/2739673014.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train_stats = y_train_stats.append(y_train[train_mask])\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Ridge was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Finished analyzing cluster 7--------\n",
      "Mean absolute error:  0.30285893799141556\n",
      "Median absolute error:  0.23698480946936984\n",
      "Mean squared error:  0.16641669309453822\n",
      "R2:  0.6497544383974037\n",
      "------------TRAIN--------------------\n",
      "Mean absolute error:  0.26308396204087475\n",
      "Median absolute error:  0.202166231186784\n",
      "Mean squared error:  0.1359019473694364\n",
      "R2:  0.7164352820579246\n",
      "--------------------------------------------------\n",
      "Coefficients: \n",
      " [[ 2.82226541e-01 -2.31491921e-02 -1.05265401e-01  2.96626908e-02\n",
      "   8.68979539e-02  1.56279216e-02  7.98136445e-01  1.30763584e-02\n",
      "   9.48794032e-03 -1.80338696e-03 -2.21801403e-01 -5.75386973e-02\n",
      "  -4.09845036e-02  1.21756459e-01 -2.43607747e-03  7.82564700e-02\n",
      "   5.42947307e-02 -5.69086640e-02  1.66036282e-02  5.42382654e-03\n",
      "  -3.12244923e-02 -1.15415348e-02  2.06374462e-01  3.40077866e-02\n",
      "  -1.72522256e-02  1.44056786e-01 -6.71329973e-02  1.25939559e-01\n",
      "  -2.51654948e-02 -1.47996426e-01  4.37135119e-02  7.24157440e-02\n",
      "   5.09636328e-02 -2.15969262e-02 -3.66455014e-01  4.21215900e-02\n",
      "  -2.37033757e-03 -9.54348607e-02 -8.45866127e-03  2.30983128e-01\n",
      "   1.81033502e-02 -4.15757353e-01  3.87973826e-01 -7.55765493e-02\n",
      "  -7.69145350e-03  2.97743347e-01  5.90359777e-02  1.48188703e-01\n",
      "  -3.22093399e-01 -2.14522349e-02 -5.61643720e-02  1.01310192e-01\n",
      "   4.57564177e-01  1.04277032e-01  5.95276975e-02 -3.43098892e-02\n",
      "  -4.66111151e-02  9.75763104e-02 -8.35997226e-03  1.32002141e+00\n",
      "   2.75180175e-02  1.17851375e-01  1.31524315e-03 -5.22700297e-02\n",
      "  -8.91670350e-02  1.38301304e-02 -9.84262851e-01  4.60987624e-02\n",
      "   2.36737611e-02 -1.54106993e-02 -4.28241965e-02  1.10121455e-01\n",
      "   1.31559387e-02 -1.50257982e-01  1.03703488e-02 -6.72247958e-02\n",
      "   4.08565557e-03 -8.15788322e-02 -3.39417327e-02  1.05014455e-01\n",
      "  -5.50986594e-02  1.02370560e-02  1.19502060e-02 -2.09759263e-02\n",
      "   1.47109286e-01  3.39904614e-01 -8.65593906e-02  8.62013265e-02\n",
      "   2.85225514e-02  5.69048746e-02 -4.72939133e-02 -1.26511825e-01\n",
      "   3.24529399e-02  1.13333199e-02  6.18271539e-02  2.17125226e-02\n",
      "   2.68787796e-01  1.92946378e-01 -8.61014656e-02 -1.31824567e-02\n",
      "   3.06954688e-02  2.28711978e-02 -6.59151817e-04 -1.99885168e-01\n",
      "  -2.88420457e-01  4.41090885e-02  6.73846766e-02  6.42966487e-02\n",
      "   3.47048402e-02  6.29634867e-02 -4.45675165e-02  1.69326665e-02\n",
      "   1.90042981e-02  9.92479691e-02  9.16765998e-02 -1.46090840e-01\n",
      "  -1.70969621e-02 -9.20257536e-03  2.69719517e-03  1.53325782e-01\n",
      "  -1.66876202e-01  1.60975888e-02 -1.49363938e-02  1.10244512e-01\n",
      "   3.29932967e-02  3.51499844e-02  2.35769691e-01  5.75352469e-03\n",
      "  -2.99858222e-02  3.24977021e-02  6.84286636e-03  1.37058460e-02\n",
      "   4.10101826e-02 -2.39617667e-01  7.13884832e-02 -1.02655970e-01\n",
      "   1.81296514e-02  1.83744439e-01  8.24368122e-02  9.81240950e-03\n",
      "  -1.79583537e-02  9.78763601e-02 -1.30286061e-02 -1.81734819e-01\n",
      "  -1.23780851e-02 -6.39771474e-02  4.18298526e-03  1.80206064e-01\n",
      "  -9.45969482e-02 -9.56881015e-02 -1.54012087e-01 -1.61641035e-01\n",
      "   9.05375359e-02 -6.59230696e-02 -3.16585270e-02  3.79380931e-02\n",
      "   1.76118422e-01  1.00237283e-02 -5.90341888e-02 -9.45306799e-02\n",
      "  -3.07462750e-02 -8.17460723e-02 -5.21827611e-02 -3.98351066e-02\n",
      "  -3.05371551e-02  3.25249412e-02  3.58565239e-01  1.56380801e-01\n",
      "   4.17525223e-02  5.69831029e-01  1.58752972e-02  8.52200510e-03\n",
      "  -2.06932859e-02  3.65317197e-02  7.71647328e-02 -8.28965638e-02\n",
      "  -2.19100873e-02 -2.76652185e-01 -8.42999141e-02  2.16067793e-02\n",
      "  -6.48573643e-02  1.30512431e+00  1.34452530e-01  1.91485269e-01\n",
      "  -3.12870466e-01 -4.82000462e-03 -2.72562987e-02 -4.98562256e-02\n",
      "  -4.07379483e-02 -9.29402786e-03 -1.68403350e-01 -8.92289113e-02\n",
      "   2.10416147e-02 -1.63093224e-02 -4.40813105e-02 -7.85452553e-02\n",
      "   1.19767104e-01 -9.63242371e-02 -3.42858805e-01]]\n",
      "Mean squared error: 0.19\n",
      "Variance score: 0.59\n",
      "R2: 0.5937473122610771\n",
      "--------- For Model:  Linear Model Ridge  ---------\n",
      "\n",
      "Mean absolute error:  0.3335109308525844\n",
      "Median absolute error:  0.2701229574207509\n",
      "Mean squared error:  0.1930280816263124\n",
      "R2:  0.5937473122610771\n",
      "--------- For Model:  Linear Model Ridge  --------- (Train Data)\n",
      "\n",
      "Mean absolute error:  0.2759400330062243\n",
      "Median absolute error:  0.21176209232483512\n",
      "Mean squared error:  0.14999741306878916\n",
      "R2:  0.6870245426780557\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------- For Model:  Linear Model Lasso  ---------\n",
      "\n",
      "Mean absolute error:  0.28811861658372334\n",
      "Median absolute error:  0.22515350941741463\n",
      "Mean squared error:  0.1605539990634289\n",
      "R2:  0.6557137690445795\n",
      "--------- For Model:  Linear Model Lasso  --------- (Train Data)\n",
      "\n",
      "Mean absolute error:  0.29067462904100566\n",
      "Median absolute error:  0.22488670181534687\n",
      "Mean squared error:  0.16582226319050164\n",
      "R2:  0.6551532756965115\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "112/112 [==============================] - 1s 2ms/step - loss: 3.2399\n",
      "Epoch 2/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5670\n",
      "Epoch 3/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.3190\n",
      "Epoch 4/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.2592\n",
      "Epoch 5/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.2304\n",
      "Epoch 6/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.2126\n",
      "Epoch 7/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.2000\n",
      "Epoch 8/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1913\n",
      "Epoch 9/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1847\n",
      "Epoch 10/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1790\n",
      "Epoch 11/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1755\n",
      "Epoch 12/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1717\n",
      "Epoch 13/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1692\n",
      "Epoch 14/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1663\n",
      "Epoch 15/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1643\n",
      "Epoch 16/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1627\n",
      "Epoch 17/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1611\n",
      "Epoch 18/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1597\n",
      "Epoch 19/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1587\n",
      "Epoch 20/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1574\n",
      "Epoch 21/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1562\n",
      "Epoch 22/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1558\n",
      "Epoch 23/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1551\n",
      "Epoch 24/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1544\n",
      "Epoch 25/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1535\n",
      "Epoch 26/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1525\n",
      "Epoch 27/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1534\n",
      "Epoch 28/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1514\n",
      "Epoch 29/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1521\n",
      "Epoch 30/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1507\n",
      "Epoch 31/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1508\n",
      "Epoch 32/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1504\n",
      "Epoch 33/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1495\n",
      "Epoch 34/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1491\n",
      "Epoch 35/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1495\n",
      "Epoch 36/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1484\n",
      "Epoch 37/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1486\n",
      "Epoch 38/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1473\n",
      "Epoch 39/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1470\n",
      "Epoch 40/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1470\n",
      "Epoch 41/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1459\n",
      "Epoch 42/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1459\n",
      "Epoch 43/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1454\n",
      "Epoch 44/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1446\n",
      "Epoch 45/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1441\n",
      "Epoch 46/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1441\n",
      "Epoch 47/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1429\n",
      "Epoch 48/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1434\n",
      "Epoch 49/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1423\n",
      "Epoch 50/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1417\n",
      "Epoch 51/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1419\n",
      "Epoch 52/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1410\n",
      "Epoch 53/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1406\n",
      "Epoch 54/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1409\n",
      "Epoch 55/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1396\n",
      "Epoch 56/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1392\n",
      "Epoch 57/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1397\n",
      "Epoch 58/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1390\n",
      "Epoch 59/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1378\n",
      "Epoch 60/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1376\n",
      "Epoch 61/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1377\n",
      "Epoch 62/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1366\n",
      "Epoch 63/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1363\n",
      "Epoch 64/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1360\n",
      "Epoch 65/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1356\n",
      "Epoch 66/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1356\n",
      "Epoch 67/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1353\n",
      "Epoch 68/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1350\n",
      "Epoch 69/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1341\n",
      "Epoch 70/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1340\n",
      "Epoch 71/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1338\n",
      "Epoch 72/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1330\n",
      "Epoch 73/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1331\n",
      "Epoch 74/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1336\n",
      "Epoch 75/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1328\n",
      "Epoch 76/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1339\n",
      "Epoch 77/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1319\n",
      "Epoch 78/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1322\n",
      "Epoch 79/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1316\n",
      "Epoch 80/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1325\n",
      "Epoch 81/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1320\n",
      "Epoch 82/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1311\n",
      "Epoch 83/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1313\n",
      "Epoch 84/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1309\n",
      "Epoch 85/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1306\n",
      "Epoch 86/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1312\n",
      "Epoch 87/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1301\n",
      "Epoch 88/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1300\n",
      "Epoch 89/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1300\n",
      "Epoch 90/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1295\n",
      "Epoch 91/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1296\n",
      "Epoch 92/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1298\n",
      "Epoch 93/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1299\n",
      "Epoch 94/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1291\n",
      "Epoch 95/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1293\n",
      "Epoch 96/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1299\n",
      "Epoch 97/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1285\n",
      "Epoch 98/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1287\n",
      "Epoch 99/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1284\n",
      "Epoch 100/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1282\n",
      "Epoch 101/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1287\n",
      "Epoch 102/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1280\n",
      "Epoch 103/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1280\n",
      "Epoch 104/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1282\n",
      "Epoch 105/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1279\n",
      "Epoch 106/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1276\n",
      "Epoch 107/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1288\n",
      "Epoch 108/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1274\n",
      "Epoch 109/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1280\n",
      "Epoch 110/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1272\n",
      "Epoch 111/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1271\n",
      "Epoch 112/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1269\n",
      "Epoch 113/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1270\n",
      "Epoch 114/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1266\n",
      "Epoch 115/300\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.1267\n",
      "Epoch 116/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1271\n",
      "Epoch 117/300\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.1273\n",
      "Epoch 118/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1265\n",
      "Epoch 119/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1260\n",
      "Epoch 120/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1261\n",
      "Epoch 121/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1268\n",
      "Epoch 122/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1272\n",
      "Epoch 123/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1265\n",
      "Epoch 124/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1260\n",
      "Epoch 125/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1257\n",
      "Epoch 126/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1257\n",
      "Epoch 127/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1257\n",
      "Epoch 128/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1252\n",
      "Epoch 129/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1251\n",
      "Epoch 130/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1253\n",
      "Epoch 131/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1254\n",
      "Epoch 132/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1253\n",
      "Epoch 133/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1252\n",
      "Epoch 134/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1251\n",
      "Epoch 135/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1248\n",
      "Epoch 136/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1251\n",
      "Epoch 137/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1247\n",
      "Epoch 138/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1243\n",
      "Epoch 139/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1242\n",
      "Epoch 140/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1245\n",
      "Epoch 141/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1243\n",
      "Epoch 142/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1241\n",
      "Epoch 143/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1238\n",
      "Epoch 144/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1238\n",
      "Epoch 145/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1240\n",
      "Epoch 146/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1237\n",
      "Epoch 147/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1239\n",
      "Epoch 148/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1235\n",
      "Epoch 149/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1233\n",
      "Epoch 150/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1236\n",
      "Epoch 151/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1236\n",
      "Epoch 152/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1235\n",
      "Epoch 153/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1237\n",
      "Epoch 154/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1234\n",
      "Epoch 155/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1232\n",
      "Epoch 156/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1232\n",
      "Epoch 157/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1231\n",
      "Epoch 158/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1240\n",
      "Epoch 159/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1233\n",
      "Epoch 160/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1229\n",
      "Epoch 161/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1233\n",
      "Epoch 162/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1229\n",
      "Epoch 163/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1230\n",
      "Epoch 164/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1227\n",
      "Epoch 165/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1226\n",
      "Epoch 166/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1227\n",
      "Epoch 167/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1228\n",
      "Epoch 168/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1223\n",
      "Epoch 169/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1227\n",
      "Epoch 170/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1225\n",
      "Epoch 171/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1228\n",
      "Epoch 172/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1226\n",
      "Epoch 173/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1239\n",
      "Epoch 174/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1226\n",
      "Epoch 175/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1224\n",
      "Epoch 176/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1219\n",
      "Epoch 177/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1220\n",
      "Epoch 178/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1219\n",
      "Epoch 179/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1218\n",
      "Epoch 180/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1215\n",
      "Epoch 181/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1222\n",
      "Epoch 182/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1215\n",
      "Epoch 183/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1215\n",
      "Epoch 184/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1217\n",
      "Epoch 185/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1220\n",
      "Epoch 186/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1215\n",
      "Epoch 187/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1223\n",
      "Epoch 188/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1214\n",
      "Epoch 189/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1211\n",
      "Epoch 190/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1214\n",
      "Epoch 191/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1216\n",
      "Epoch 192/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1211\n",
      "Epoch 193/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1211\n",
      "Epoch 194/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1212\n",
      "Epoch 195/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1212\n",
      "Epoch 196/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1211\n",
      "Epoch 197/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1210\n",
      "Epoch 198/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1210\n",
      "Epoch 199/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1212\n",
      "Epoch 200/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1207\n",
      "Epoch 201/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1205\n",
      "Epoch 202/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1210\n",
      "Epoch 203/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1208\n",
      "Epoch 204/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1206\n",
      "Epoch 205/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1210\n",
      "Epoch 206/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1205\n",
      "Epoch 207/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1204\n",
      "Epoch 208/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1205\n",
      "Epoch 209/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1207\n",
      "Epoch 210/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1206\n",
      "Epoch 211/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1204\n",
      "Epoch 212/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1202\n",
      "Epoch 213/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1201\n",
      "Epoch 214/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1200\n",
      "Epoch 215/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1203\n",
      "Epoch 216/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1204\n",
      "Epoch 217/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1203\n",
      "Epoch 218/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1208\n",
      "Epoch 219/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1201\n",
      "Epoch 220/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1204\n",
      "Epoch 221/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1197\n",
      "Epoch 222/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1202\n",
      "Epoch 223/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1197\n",
      "Epoch 224/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1198\n",
      "Epoch 225/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1200\n",
      "Epoch 226/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1198\n",
      "Epoch 227/300\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1200\n",
      "Epoch 228/300\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1196\n",
      "Epoch 229/300\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1195\n",
      "Epoch 230/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1196\n",
      "Epoch 231/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1195\n",
      "Epoch 232/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1196\n",
      "Epoch 233/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1194\n",
      "Epoch 234/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1194\n",
      "Epoch 235/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1194\n",
      "Epoch 236/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1197\n",
      "Epoch 237/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1194\n",
      "Epoch 238/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1191\n",
      "Epoch 239/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1195\n",
      "Epoch 240/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1190\n",
      "Epoch 241/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1190\n",
      "Epoch 242/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1196\n",
      "Epoch 243/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1193\n",
      "Epoch 244/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1193\n",
      "Epoch 245/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1192\n",
      "Epoch 246/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1188\n",
      "Epoch 247/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1189\n",
      "Epoch 248/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1195\n",
      "Epoch 249/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1187\n",
      "Epoch 250/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1187\n",
      "Epoch 251/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1191\n",
      "Epoch 252/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1189\n",
      "Epoch 253/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1186\n",
      "Epoch 254/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1186\n",
      "Epoch 255/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1187\n",
      "Epoch 256/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1187\n",
      "Epoch 257/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1193\n",
      "Epoch 258/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1186\n",
      "Epoch 259/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1185\n",
      "Epoch 260/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1184\n",
      "Epoch 261/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1185\n",
      "Epoch 262/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1187\n",
      "Epoch 263/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1184\n",
      "Epoch 264/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1186\n",
      "Epoch 265/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1182\n",
      "Epoch 266/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1183\n",
      "Epoch 267/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1184\n",
      "Epoch 268/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1183\n",
      "Epoch 269/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1184\n",
      "Epoch 270/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1182\n",
      "Epoch 271/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1184\n",
      "Epoch 272/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1183\n",
      "Epoch 273/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1180\n",
      "Epoch 274/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1182\n",
      "Epoch 275/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1181\n",
      "Epoch 276/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1180\n",
      "Epoch 277/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1180\n",
      "Epoch 278/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1180\n",
      "Epoch 279/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1180\n",
      "Epoch 280/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1178\n",
      "Epoch 281/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1178\n",
      "Epoch 282/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1178\n",
      "Epoch 283/300\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1181\n",
      "Epoch 284/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1176\n",
      "Epoch 285/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1186\n",
      "Epoch 286/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1177\n",
      "Epoch 287/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1177\n",
      "Epoch 288/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1179\n",
      "Epoch 289/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1176\n",
      "Epoch 290/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1175\n",
      "Epoch 291/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1176\n",
      "Epoch 292/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "Epoch 293/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "Epoch 294/300\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.1175\n",
      "Epoch 295/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1174\n",
      "Epoch 296/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1176\n",
      "Epoch 297/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "Epoch 298/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1173\n",
      "Epoch 299/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1172\n",
      "Epoch 300/300\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "finished fitting\n",
      "--------- For Model:  NN  ---------\n",
      "\n",
      "99/99 [==============================] - 0s 1ms/step\n",
      "Mean absolute error:  0.37712550913048637\n",
      "Median absolute error:  0.3080296519219785\n",
      "Mean squared error:  0.242806684413028\n",
      "R2:  0.48898177243075847\n",
      "--------- For Model:  NN  --------- (Train Data)\n",
      "\n",
      "889/889 [==============================] - 1s 1ms/step\n",
      "Mean absolute error:  0.24509231653440156\n",
      "Median absolute error:  0.18898351922475487\n",
      "Mean squared error:  0.11626251301654138\n",
      "R2:  0.7574137284350148\n",
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #coeffs = np.load('../Data/selected_coefs_pvals.npy')\n",
    "    coeffs = np.load('../Data/selected_coefs.npy')\n",
    "    col_set = set()\n",
    "    cherry_picked_list = [\n",
    "    'host_identity_verified',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'accommodates',\n",
    "    'bathrooms',\n",
    "    'bedrooms',\n",
    "    'beds',\n",
    "    'guests_included',\n",
    "    'security_deposit',\n",
    "    'cleaning_fee',\n",
    "    'extra_people',\n",
    "    'number_of_reviews',\n",
    "    'review_scores_rating',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'reviews_per_month',\n",
    "    'comments',\n",
    "\n",
    "    ]\n",
    "    \n",
    "    for i in range(len(coeffs)):\n",
    "        if (coeffs[i]):\n",
    "            col_set.add(X_train.columns[i])\n",
    "    X_train = X_train[list(col_set)]\n",
    "    X_val = X_val[list(col_set)]\n",
    "    X_test = X_test[list(col_set)]\n",
    "\n",
    "    X_concat = pd.concat([X_train, X_val], ignore_index=True)\n",
    "    y_concat = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "    #RUN WITHOUT FEATURE SELECTION FOR THE BASELINE\n",
    "    \"\"\"\n",
    "    print(\"--------------------Linear Regression--------------------\")\n",
    "    LinearModel(X_concat, y_concat, X_test, y_test)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--------------------Tree-based Model--------------------\")\n",
    "    TreebasedModel(X_concat, y_concat, X_test, y_test)\n",
    "    print(\"--------------------KMeans Clustering--------------------\")\n",
    "    c_pred, centroids = kmeans(X_concat, y_concat, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"--------------------------------------------------\")\n",
    "    LinearModelRidge(X_concat, y_concat, X_test, y_test)\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "    print(\"--------------------------------------------------\")\n",
    "    LinearModelLasso(X_train, y_train, X_val, y_val, 0.002)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    simple_neural_network(X_concat, y_concat, X_test, y_test)\n",
    "\n",
    "    svm(X_concat, y_concat, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b486139",
   "metadata": {},
   "source": [
    "# Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5455224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_boost_sig(X, y, k=5):\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    import xgboost as xgb\n",
    "    model = xgb.XGBRegressor(colsample_bytree = 0.8, n_estimators=150, random_state=42, loss='lad', learning_rate=0.05, max_depth=6, subsample=0.8)\n",
    "\n",
    "    kf = KFold(n_splits=k)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fc031ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearModel_sig(X, y, k=5):\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    model = linear_model.LinearRegression(n_jobs=int(0.8*n_cores))\n",
    "    kf = KFold(n_splits=k)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "617c3da8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def LinearModelRidge_sig(X, y,k=5):\n",
    "    model = Ridge(alpha = 7) #7\n",
    "    kf = KFold(n_splits=k)\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c31b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearModelLasso_sig(X,y, k=5, alpha=0.002):\n",
    "    model = Lasso(alpha = alpha) #0.5\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    kf = KFold(n_splits=k)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48f04b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neural_network_sig(X, y, k=5):\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    kf = KFold(n_splits=k)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=20, activation='relu', input_dim=len(X.values[0])))\n",
    "    model.add(Dense(units=5, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    adam = optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=DECAY_RATE, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train,epochs=NUM_ITERATIONS, batch_size=BATCH_SIZE)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8714bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreebasedModel_sig(X,y,k=5):\n",
    "    kf = KFold(n_splits=k)\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    classifier_list, classifier_name_list = get_ensemble_models()\n",
    "    for classifier, classifier_name in zip(classifier_list, classifier_name_list):\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "            y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "            classifier.fit(X_train, y_train)\n",
    "            predicted_values = classifier.predict(X_test)\n",
    "            scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "            scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "            scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37639dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_SGD_sig(X,y,k=5):\n",
    "    kf = KFold(n_splits=k)\n",
    "    model = SGDRegressor()\n",
    "    scores_r2=[]\n",
    "    scores_mae = []\n",
    "    scores_mse = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34f51266",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def svm_significance(X, y, k=5):\n",
    "    kf = KFold(n_splits=k)\n",
    "    scores = []\n",
    "    model = SVR(gamma = 0.05, verbose = True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index] \n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_values = model.predict(X_test)\n",
    "        scores_mae.append(metrics.mean_absolute_error(y_test, predicted_values))\n",
    "        scores_mse.append(metrics.mean_squared_error(y_test, predicted_values))\n",
    "        scores_r2.append(metrics.r2_score(y_test, predicted_values))\n",
    "\n",
    "    mean_score_r2 = sum(scores_r2) / len(scores_r2)\n",
    "    st_dev_r2=np.std(scores_r2)\n",
    "    \n",
    "    print('R2 mean:  ' + str(mean_score_r2))\n",
    "    print('R2 std:  ' + str(st_dev_r2))\n",
    "    \n",
    "    mean_score_mae = sum(scores_mae) / len(scores_mae)\n",
    "    st_dev_mae=np.std(scores_mae)\n",
    "    \n",
    "    print('MAE mean:  ' + str(mean_score_mae))\n",
    "    print('MAE std:  ' + str(st_dev_mae))\n",
    "    \n",
    "    mean_score_mse = sum(scores_mse) / len(scores_mse)\n",
    "    st_dev_mse=np.std(scores_mse)\n",
    "    \n",
    "    print('MSE mean:  ' + str(mean_score_mse))\n",
    "    print('MSE std:  ' + str(st_dev_mse))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e601ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concat_old = X_concat.drop('EDU10_E', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d94adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:25:26] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:25:48] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:26:09] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:26:32] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:26:57] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "R2 mean:  0.7247234757455716\n",
      "R2 std:  0.009156862472011454\n",
      "MAE mean:  0.25401282610584885\n",
      "MAE std:  0.0024917175698813206\n",
      "MSE mean:  0.1320110975705568\n",
      "MSE std:  0.0077207629487990315\n",
      "[22:27:22] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:27:47] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:28:14] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:28:35] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:28:57] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-37/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"loss\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "R2 mean:  0.7210201745221593\n",
      "R2 std:  0.009369589785234736\n",
      "MAE mean:  0.2565092732537784\n",
      "MAE std:  0.0030170193233057087\n",
      "MSE mean:  0.13378628966974085\n",
      "MSE std:  0.007844662652289506\n"
     ]
    }
   ],
   "source": [
    "xg_new = xg_boost_sig(X_concat, y_concat, k=5)\n",
    "xg_old = xg_boost_sig(X_concat_old, y_concat, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d03947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "252558c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6817847785462273\n",
      "R2 std:  0.00842797137238802\n",
      "MAE mean:  0.27792521383523955\n",
      "MAE std:  0.002605091912297446\n",
      "MSE mean:  0.15256878129800153\n",
      "MSE std:  0.007716869887086794\n",
      "R2 mean:  0.6796225807183897\n",
      "R2 std:  0.008393974102545703\n",
      "MAE mean:  0.2792111701697516\n",
      "MAE std:  0.002842824518554625\n",
      "MSE mean:  0.15361048532357274\n",
      "MSE std:  0.007866634858221163\n"
     ]
    }
   ],
   "source": [
    "lin_new = LinearModel_sig(X_concat, y_concat, k=5)\n",
    "lin_old = LinearModel_sig(X_concat_old, y_concat, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce57faac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6816042668907638\n",
      "R2 std:  0.00897117842417922\n",
      "MAE mean:  0.2783543233350677\n",
      "MAE std:  0.002936122547603586\n",
      "MSE mean:  0.1526566262990877\n",
      "MSE std:  0.00787518067441251\n",
      "R2 mean:  0.6794035330188143\n",
      "R2 std:  0.008891681666679406\n",
      "MAE mean:  0.2796390142212753\n",
      "MAE std:  0.003099758022632467\n",
      "MSE mean:  0.15371770001510723\n",
      "MSE std:  0.008030407525714447\n"
     ]
    }
   ],
   "source": [
    "linr_new = LinearModelRidge_sig(X_concat, y_concat,k=5)\n",
    "linr_old = LinearModelRidge_sig(X_concat_old, y_concat,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db1aac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6542038656537376\n",
      "R2 std:  0.009548471106548558\n",
      "MAE mean:  0.29107077894869093\n",
      "MAE std:  0.00355834447276121\n",
      "MSE mean:  0.16578766504390136\n",
      "MSE std:  0.008359527097508451\n",
      "R2 mean:  0.644905400434026\n",
      "R2 std:  0.009165985068889536\n",
      "MAE mean:  0.29602419518906886\n",
      "MAE std:  0.003870198777560502\n",
      "MSE mean:  0.17025416947019967\n",
      "MSE std:  0.00862931834984352\n"
     ]
    }
   ],
   "source": [
    "linl_new  =LinearModelLasso_sig(X_concat,y_concat, alpha=0.002)\n",
    "linl_old  =LinearModelLasso_sig(X_concat_old,y_concat, alpha=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cadbdab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 1s 2ms/step - loss: 12.8322\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 1.3552\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.8650\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.6670\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.5451\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.4600\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.4000\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.3577\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.3271\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.3043\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2867\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2728\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2616\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2438\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2367\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2304\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2250\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2200\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2156\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2114\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2075\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2040\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.2007\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1976\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1946\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1916\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1891\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1867\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1839\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1820\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1796\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1777\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1755\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1737\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1722\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1703\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1690\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1674\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1664\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1652\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1639\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1627\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1618\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1605\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1601\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1587\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1581\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1571\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1566\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1558\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1549\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1541\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1534\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1526\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1526\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1517\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1519\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1504\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1499\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1490\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1482\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1478\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1467\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1464\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1460\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1452\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1445\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1441\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1436\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1430\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1423\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1421\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1415\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1414\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1406\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1405\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1398\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1394\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1390\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1390\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1382\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1380\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1374\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1374\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1372\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1362\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1364\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1357\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1354\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1355\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1349\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1345\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1343\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1337\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1334\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1329\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1328\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1328\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1322\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1319\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1317\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1310\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1307\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1303\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1300\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1300\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1296\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1291\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1291\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1286\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1283\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1280\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1283\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1278\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1274\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1270\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1268\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1265\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1263\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1263\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1258\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1255\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1257\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1256\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1249\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1247\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1251\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1245\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1244\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1242\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1237\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1239\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1233\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1232\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1231\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1228\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1226\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1222\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1220\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1220\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1218\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1217\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1215\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1213\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1212\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1215\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1209\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1209\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1213\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1207\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1205\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1203\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1200\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1199\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1198\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1197\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1195\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1196\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1192\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1189\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1188\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1192\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1189\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1184\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1183\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1185\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1184\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1183\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1180\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1176\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1176\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1175\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1171\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1172\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1171\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1170\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1167\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1167\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1168\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1164\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1165\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1163\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1161\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1162\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1160\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1159\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1158\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1158\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1155\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1154\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1152\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1158\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1153\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1151\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1149\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1149\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 201/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1148\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1145\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1143\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1146\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1144\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1140\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1142\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1140\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1140\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1139\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1138\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1136\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1136\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1135\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1136\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1133\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1137\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1131\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1135\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1132\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1130\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1129\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1129\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1128\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1128\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1127\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1127\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1128\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1124\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1126\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1123\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1124\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1124\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1124\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1122\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1123\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1122\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1120\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1120\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1118\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1120\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1119\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1119\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1116\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1115\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1116\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1117\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1117\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1113\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1114\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1111\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1109\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1109\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1109\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1105\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1106\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1104\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1106\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1102\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1102\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1102\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1097\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1096\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1094\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "178/178 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1232\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1218\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1209\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1201\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1196\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1195\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1187\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1185\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1182\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1177\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1173\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1170\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1166\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1166\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1164\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1162\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1159\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1160\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1158\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1155\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1155\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1152\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1154\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1149\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1149\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1148\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1147\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1145\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1143\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1142\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1142\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1140\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1140\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1141\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1137\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1137\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1135\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1135\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1133\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1134\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1132\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1132\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1130\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1131\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1129\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1127\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1130\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1125\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1125\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1126\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1125\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1123\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1123\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1122\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1120\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1121\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1122\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1121\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1118\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1118\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1118\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1121\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1116\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1116\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1114\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1114\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1113\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1113\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1111\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1112\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1109\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1107\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1107\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1108\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1107\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1105\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1104\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1104\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1102\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1101\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 1s 5ms/step - loss: 0.1102\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1100\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1101\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1099\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1100\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1098\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1097\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1094\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1094\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1092\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1091\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1092\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1090\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1088\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1089\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1089\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1088\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1089\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1088\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1088\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1087\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1087\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1087\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1087\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1085\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1084\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1084\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1080\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1080\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1080\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1079\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1077\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1078\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1078\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1078\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1076\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1077\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1076\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1075\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1074\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1074\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1074\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1074\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1070\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1067\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1067\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1067\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1066\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1064\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1066\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1063\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1064\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1063\n",
      "Epoch 201/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1064\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1063\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1059\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1057\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1052\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1054\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1043\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1043\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1043\n",
      "178/178 [==============================] - 0s 1ms/step\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1114\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1106\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1092\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1089\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1084\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1083\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1080\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1076\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1075\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1074\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1072\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1072\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1070\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1068\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1066\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1065\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1065\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1061\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1060\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1055\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1051\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1050\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1049\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1047\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1044\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1043\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1043\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1041\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1041\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1041\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1039\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1041\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1039\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1038\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1036\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1036\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1034\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1033\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1032\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1032\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1032\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1033\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1033\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1031\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1030\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1030\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1031\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1029\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1029\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1028\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1028\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1029\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1028\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1027\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1028\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1026\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1026\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1027\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1026\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1027\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1026\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1026\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1025\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1025\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1026\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1025\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1026\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1025\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1024\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1024\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1024\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1025\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1024\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1023\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1024\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1023\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1023\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1023\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1024\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1022\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1023\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1022\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1023\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1021\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1022\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1023\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1022\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1021\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1021\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1019\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1019\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1019\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1019\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1018\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1018\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1018\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1017\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1018\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1018\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1017\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1017\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1017\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1018\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1016\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1015\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1017\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1015\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1016\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1015\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1015\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1015\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1015\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1014\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1014\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1014\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1014\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1014\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1014\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1013\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1014\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1013\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1014\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1013\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1013\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1013\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1013\n",
      "Epoch 201/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1012\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1012\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1012\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1011\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1010\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1010\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1011\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1010\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1009\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1010\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1009\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1009\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1009\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1008\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1008\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1009\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1007\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1007\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1007\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1007\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1007\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1007\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1006\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1006\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1006\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1007\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1008\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1005\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1004\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1004\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1004\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1004\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1004\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1003\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1003\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1004\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1003\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1001\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1002\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1001\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1001\n",
      "178/178 [==============================] - 0s 2ms/step\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1128\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1121\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1117\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1113\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1110\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1108\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1107\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1104\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1103\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1101\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1098\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1095\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1094\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1093\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1091\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1090\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1089\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1087\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1087\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1085\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1084\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1082\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1082\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1082\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1081\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1081\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1080\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1079\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1078\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1078\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1075\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1075\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1075\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1073\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1073\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1071\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1072\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1070\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1070\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1069\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1071\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1069\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1069\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1069\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1068\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1067\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1066\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1064\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1061\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1059\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1059\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1059\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1052\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1052\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1052\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1049\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1049\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 201/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1041\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1041\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1040\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1039\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1039\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1039\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1038\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1038\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1038\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1038\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1038\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1037\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1037\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1035\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1034\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1034\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1034\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1032\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1034\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1032\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "178/178 [==============================] - 0s 2ms/step\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1116\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1111\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1108\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1105\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1100\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1099\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1096\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1094\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1092\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1091\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1090\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1090\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1089\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1089\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1088\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1087\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1086\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1086\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1084\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1084\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1083\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1082\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1082\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1080\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1080\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1079\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1078\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1074\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1074\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1074\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1074\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1072\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1071\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1072\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1071\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1071\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1071\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1070\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1070\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1070\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1070\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1069\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1069\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1068\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1070\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1068\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1068\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1069\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1069\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1068\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1066\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1066\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1066\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1065\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1065\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1064\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1064\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1062\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1062\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1061\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1061\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1062\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1061\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1059\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1060\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1060\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1056\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1056\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1055\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1054\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1054\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1055\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1054\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1055\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1055\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1054\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1054\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1053\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1054\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1052\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1052\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1051\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1051\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1052\n",
      "Epoch 201/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1051\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1051\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1050\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1051\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1050\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1050\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1051\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1050\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1050\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1050\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1050\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1050\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1049\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1049\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1050\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1049\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1049\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1049\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1049\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1048\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1049\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1049\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1048\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1048\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1049\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1047\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1046\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1047\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1047\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1046\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1046\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1046\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1046\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1045\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1045\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1045\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1045\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1044\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1044\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1044\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1044\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1044\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1045\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1044\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1045\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1043\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1044\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1043\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "178/178 [==============================] - 0s 2ms/step\n",
      "R2 mean:  0.7132547629163465\n",
      "R2 std:  0.043547991297281506\n",
      "MAE mean:  0.2657395140942998\n",
      "MAE std:  0.015746985438791918\n",
      "MSE mean:  0.13743230888905475\n",
      "MSE std:  0.02102784192000268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "89/89 [==============================] - 1s 4ms/step - loss: 5.1429\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7138\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.4212\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.3119\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.2683\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.2459\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.2315\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 5ms/step - loss: 0.2210\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.2121\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.2048\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1990\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1937\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1891\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1850\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1818\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1787\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1760\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1738\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1715\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1699\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1675\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1658\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1643\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1641\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1624\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1611\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1598\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1587\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1577\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1570\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1564\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1559\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1557\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1543\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1539\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1531\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1524\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1524\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1517\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1509\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1509\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1502\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1494\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1492\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1489\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1492\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1484\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1479\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1483\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1470\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1460\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1457\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1454\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1449\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1445\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1450\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1440\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1431\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1425\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1425\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1420\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1416\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1406\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1404\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1404\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1391\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1393\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1381\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1380\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1372\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1362\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1358\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1355\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1348\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1346\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1340\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1338\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1337\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1337\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1330\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1322\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1313\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1307\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1307\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1304\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1300\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1293\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1289\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1287\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1285\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1279\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1280\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1275\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1269\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1265\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1262\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1259\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1258\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1259\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1252\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1253\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1249\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1244\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 0.1242\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1240\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1236\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1232\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1236\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1235\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1228\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1230\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1227\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 0.1220\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1221\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 0.1214\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1214\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1215\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1212\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1208\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1208\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1208\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1207\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1206\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1204\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1198\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1200\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1195\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1198\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1200\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1192\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1193\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1195\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1189\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1192\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1189\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1187\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1187\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1185\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1187\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1180\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1179\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1178\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1179\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1174\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1173\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1173\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1168\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1173\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1171\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1168\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1167\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1169\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1167\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1167\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1163\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1166\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1161\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1160\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1158\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1160\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1162\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1162\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1156\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1158\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1157\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1154\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1152\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1151\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1150\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1150\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1150\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1147\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1151\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1146\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1150\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1148\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1145\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1150\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1143\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1150\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1143\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1139\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1142\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1142\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1142\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1139\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1139\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1137\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1137\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1137\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1135\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1133\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1136\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1133\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1136\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1138\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1132\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1130\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1130\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1134\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1129\n",
      "Epoch 202/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1127\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1127\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1130\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1126\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1126\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1125\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1125\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 1s 5ms/step - loss: 0.1125\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1126\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1125\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1123\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1122\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1121\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1121\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1119\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1120\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1123\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1119\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1120\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1117\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1124\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1122\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1117\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1118\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1114\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1113\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1115\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1116\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1113\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1114\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1116\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1112\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1111\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1113\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1111\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1112\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1112\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1115\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1112\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1107\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1113\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1107\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1108\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1108\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1106\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1108\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1105\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1105\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1105\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1103\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1107\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1103\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1102\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1104\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1102\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1103\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1100\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1102\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1098\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1099\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1100\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1101\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1097\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1100\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1096\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1095\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1094\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1095\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1093\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1092\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1093\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1090\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1095\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1092\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1092\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1094\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1092\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1092\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1088\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1090\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1088\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1089\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1087\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 1s 4ms/step\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1230\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1216\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1202\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1202\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1192\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1188\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1184\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1182\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1181\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1177\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1177\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1172\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1171\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1167\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1171\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1164\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1164\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1164\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1164\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1158\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1157\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1154\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1154\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1156\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1153\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1153\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1150\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1150\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1147\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1146\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1148\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1147\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1144\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1145\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1140\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1139\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1141\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1142\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1141\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1141\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1135\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1136\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1134\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1134\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1134\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1131\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1130\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1131\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1131\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1130\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1134\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1131\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1128\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1124\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1128\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1127\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1127\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1124\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1128\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1122\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1123\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1125\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1122\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1120\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1119\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1119\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1121\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1120\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1117\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1117\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1117\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1116\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1115\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1116\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1115\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1114\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1115\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1113\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1110\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1111\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1109\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1109\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1110\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1109\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1108\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1106\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1106\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1105\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1106\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1104\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1105\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1104\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1108\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1102\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1102\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1105\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1100\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1100\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 102/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1100\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1100\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1098\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1102\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1104\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1095\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1095\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1096\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1096\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1099\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1095\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1095\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1095\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1097\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1094\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1091\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1092\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1091\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1090\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1088\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1090\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1088\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1087\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1088\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1087\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1088\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1088\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1089\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1086\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1085\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1086\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1083\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1086\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1084\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1085\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1086\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1081\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1083\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1081\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1081\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1079\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1080\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1081\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1080\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1080\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1077\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1076\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1077\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1074\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1077\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1071\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1071\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1071\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1073\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1072\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1071\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1070\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1069\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1070\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1068\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1069\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1069\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1069\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1068\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 202/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1068\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1066\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1065\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1066\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1062\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1062\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1064\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1061\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1059\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1059\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1057\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1056\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1055\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1055\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1053\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1052\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1051\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1052\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1050\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1049\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1049\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1049\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1048\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 1s 4ms/step\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1112\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 5ms/step - loss: 0.1106\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1100\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1094\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1090\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1092\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1088\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1086\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1081\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1081\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1079\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1082\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1077\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1076\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1073\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 5ms/step - loss: 0.1073\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1073\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1070\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1071\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1069\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1069\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1068\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1067\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1066\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1064\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1063\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1064\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1062\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1061\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1060\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1059\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1058\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1059\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1059\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1056\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1055\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1056\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1055\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1054\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1053\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1050\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1046\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1046\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1046\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1044\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1043\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1044\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1043\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1043\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1043\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1042\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1042\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1042\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1042\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1041\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1042\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1041\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1039\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1038\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1040\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1040\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 0.1040\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 0.1039\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 0.1040\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1037\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1040\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1037\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1035\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1035\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1037\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1035\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1035\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1035\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1035\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1034\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1035\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1034\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1033\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1033\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1035\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1031\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1031\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1035\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1031\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1032\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1031\n",
      "Epoch 102/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1032\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1031\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1032\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1029\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1029\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1029\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1030\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1029\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1031\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1029\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1028\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1027\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1027\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1029\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1029\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1027\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1027\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1026\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1025\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1025\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1026\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1024\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1026\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1026\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1024\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1025\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1024\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1022\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1026\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1023\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1022\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1022\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1023\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1023\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1022\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1023\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1022\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1021\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1021\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1021\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1020\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1020\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1020\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1021\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1019\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1019\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1021\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1021\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1020\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1019\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1018\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1019\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1018\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1018\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1018\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1017\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1017\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1017\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1016\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1018\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1016\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1016\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1016\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1017\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1015\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1015\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1016\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1015\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1017\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1015\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1014\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1015\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1013\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1015\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1014\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1014\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1013\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1012\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1012\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1012\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1013\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1012\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1011\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1013\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1012\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1011\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1011\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1010\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1011\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1010\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1011\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1011\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1011\n",
      "Epoch 202/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1011\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1011\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1010\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1008\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1008\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1009\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1008\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1007\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1007\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1008\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1007\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1006\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1006\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1007\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1009\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1006\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1006\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1005\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1005\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1005\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1005\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1005\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1005\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1005\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1005\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1005\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1005\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1006\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1003\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1004\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1003\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1004\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1004\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1002\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1002\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1003\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1003\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1002\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1002\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1003\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1001\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1002\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1001\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1000\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1001\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1001\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1003\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1000\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1002\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1000\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1001\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1001\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1000\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0999\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1000\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1000\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1000\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1000\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1002\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1000\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1002\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1000\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0999\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0999\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0999\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.0999\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0999\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0999\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0998\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0999\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0998\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0997\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0999\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0999\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0999\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0998\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.0997\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0999\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.0998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 0s 2ms/step\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1115\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1110\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1108\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1103\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1101\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1101\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1098\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1095\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1095\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1094\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1092\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1090\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1089\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1089\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1088\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1088\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1085\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1085\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1084\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1083\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1081\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1083\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1077\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1073\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1071\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1070\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1070\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1069\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1068\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1071\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1067\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1069\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1067\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1066\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1065\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1064\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1063\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1061\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1062\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1060\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1060\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1059\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1058\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1058\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1057\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1057\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 102/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1054\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1057\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1056\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1052\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1052\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1052\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1050\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1047\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1047\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1046\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1046\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1045\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1046\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1043\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1042\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1042\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1042\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1040\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1041\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 202/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1039\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1039\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1039\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1038\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 2ms/step - loss: 0.1036\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1039\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1036\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1034\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1033\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1035\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1034\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1033\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1033\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1033\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1033\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1033\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1033\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1033\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1033\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1034\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1032\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1033\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1032\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1032\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1033\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 0.1032\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1031\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1034\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1032\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 0.1032\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 0.1031\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1034\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 1s 15ms/step - loss: 0.1032\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1032\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.1032\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1032\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1031\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1033\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1031\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1031\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1031\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1031\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1032\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 1s 4ms/step\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1106\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1101\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1099\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1096\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1093\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1092\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1090\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1089\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1088\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1087\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1085\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1083\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1082\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1082\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1082\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1081\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1078\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1079\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1079\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1078\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1076\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1077\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1075\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1076\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1075\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1074\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1073\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1073\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1072\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1072\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1074\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1072\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1072\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1070\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1072\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1070\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.1069\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1070\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1068\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1069\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1068\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.1067\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1067\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1067\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1067\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1067\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1066\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1066\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1065\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1065\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 0.1065\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1065\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1064\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 5ms/step - loss: 0.1064\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1063\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1063\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1065\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1062\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1063\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1062\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1062\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1061\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1061\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1061\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1061\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1061\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 0.1060\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1059\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1060\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1059\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1060\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1058\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 0.1058\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1058\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1057\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1056\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1057\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1057\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 102/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1055\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1056\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1054\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 112/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 113/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1055\n",
      "Epoch 114/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 115/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 116/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1053\n",
      "Epoch 117/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 118/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 119/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 120/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 121/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 122/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 123/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 124/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 125/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 126/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1052\n",
      "Epoch 127/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 128/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1051\n",
      "Epoch 129/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1051\n",
      "Epoch 130/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1051\n",
      "Epoch 131/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1051\n",
      "Epoch 132/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 133/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 134/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1051\n",
      "Epoch 135/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1052\n",
      "Epoch 136/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1049\n",
      "Epoch 137/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 138/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 139/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 140/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1050\n",
      "Epoch 141/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 142/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 143/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 144/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 145/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 146/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1050\n",
      "Epoch 147/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 148/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1049\n",
      "Epoch 149/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 150/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 151/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 152/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 153/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 154/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 155/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 156/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 157/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 158/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 159/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 160/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 161/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 162/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1047\n",
      "Epoch 163/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 164/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 165/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 166/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 167/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 168/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1048\n",
      "Epoch 169/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 170/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 171/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 172/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 173/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 174/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 175/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 176/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 177/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 178/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1047\n",
      "Epoch 179/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 180/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 181/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 182/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 183/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1046\n",
      "Epoch 184/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 185/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 186/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 187/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 188/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1045\n",
      "Epoch 189/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 190/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 191/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 192/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 193/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1044\n",
      "Epoch 194/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 195/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 196/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1044\n",
      "Epoch 197/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 198/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 199/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 200/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 202/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 203/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1044\n",
      "Epoch 204/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 205/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 206/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1043\n",
      "Epoch 207/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 208/300\n",
      "89/89 [==============================] - 0s 3ms/step - loss: 0.1042\n",
      "Epoch 209/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 210/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1042\n",
      "Epoch 211/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 212/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1043\n",
      "Epoch 213/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1041\n",
      "Epoch 214/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1043\n",
      "Epoch 215/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 216/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 217/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 218/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 219/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 220/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 221/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 222/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 223/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 224/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 225/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 226/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1042\n",
      "Epoch 227/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 228/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 229/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 230/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 231/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 232/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 233/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 234/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 235/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 236/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 237/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1040\n",
      "Epoch 238/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 239/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 240/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 241/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 242/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 243/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 244/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 245/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1040\n",
      "Epoch 246/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 247/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 248/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 249/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 250/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 251/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 252/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 253/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1040\n",
      "Epoch 254/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 255/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 256/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 257/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 258/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 259/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 260/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 261/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 262/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1038\n",
      "Epoch 263/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1038\n",
      "Epoch 264/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 265/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 266/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 267/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 268/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 269/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 270/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 271/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 272/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1039\n",
      "Epoch 273/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 274/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 275/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 276/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 277/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 278/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 279/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 280/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 281/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 282/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 283/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 284/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1038\n",
      "Epoch 285/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 286/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.1036\n",
      "Epoch 287/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1036\n",
      "Epoch 288/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 289/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 290/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1037\n",
      "Epoch 291/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1036\n",
      "Epoch 292/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 293/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1036\n",
      "Epoch 294/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1036\n",
      "Epoch 295/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 296/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1036\n",
      "Epoch 297/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 0.1036\n",
      "Epoch 298/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 299/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n",
      "Epoch 300/300\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.1035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 1s 3ms/step\n",
      "R2 mean:  0.7145523722214202\n",
      "R2 std:  0.039906240898503156\n",
      "MAE mean:  0.26567856179425575\n",
      "MAE std:  0.014284842012995439\n",
      "MSE mean:  0.13678286810373047\n",
      "MSE std:  0.019178295688837235\n"
     ]
    }
   ],
   "source": [
    "nn_new = simple_neural_network_sig(X_concat, y_concat, k=5)\n",
    "nn_old = simple_neural_network_sig(X_concat_old, y_concat, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbd643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d1f2407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6696347030650347\n",
      "R2 std:  0.012242512532241848\n",
      "MAE mean:  0.27616456031329883\n",
      "MAE std:  0.0036469820398265314\n",
      "MSE mean:  0.158444678233524\n",
      "MSE std:  0.009853195711449625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:301: FutureWarning: The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use 'absolute_error' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6640998585956381\n",
      "R2 std:  0.010339874392652803\n",
      "MAE mean:  0.27958880253040563\n",
      "MAE std:  0.003219768610871768\n",
      "MSE mean:  0.16107395196596808\n",
      "MSE std:  0.008978664103269533\n"
     ]
    }
   ],
   "source": [
    "gb_new = TreebasedModel_sig(X_concat,y_concat)\n",
    "gb_old = TreebasedModel_sig(X_concat_old,y_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e532b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6569772889235173\n",
      "R2 std:  0.007978676526743342\n",
      "MAE mean:  0.28858794463673026\n",
      "MAE std:  0.0031302349143083456\n",
      "MSE mean:  0.16444542252639494\n",
      "MSE std:  0.007655504820401673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6514423255723344\n",
      "R2 std:  0.008969155753747867\n",
      "MAE mean:  0.29254945365215335\n",
      "MAE std:  0.004522404656886578\n",
      "MSE mean:  0.1671300762281714\n",
      "MSE std:  0.008684651221101585\n"
     ]
    }
   ],
   "source": [
    "lsgd_new = linear_model_SGD_sig(X_concat,y_concat,k=5)\n",
    "lsgd_old = linear_model_SGD_sig(X_concat_old,y_concat,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousseflahlou/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]........"
     ]
    }
   ],
   "source": [
    "svm_new = svm_significance(X_concat, y_concat, k=5)\n",
    "svm_old = svm_significance(X_concat_old, y_concat, k=5)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
